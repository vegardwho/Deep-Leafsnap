{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "007a59cd-65b9-43da-939a-a2bb39a8303c",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50f634b8-3147-429f-ae25-eb7b7b1eadab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import argparse\n",
    "# !pip install opencv-python\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "# !pip install pandas\n",
    "import pandas as pd\n",
    "# !pip install scipy\n",
    "import scipy.misc\n",
    "import shutil\n",
    "import time\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import sys\n",
    "    # caution: path[0] is reserved for script path (or '' in REPL)\n",
    "# sys.path.insert(1, 'Deep-Leafsnap-master')\n",
    "\n",
    "# !pip install scikit-image\n",
    "# !pip install scikit-learn \n",
    "import utils\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# !pip install imblearn\n",
    "import imblearn\n",
    "from collections import Counter\n",
    "\n",
    "from PIL import Image\n",
    "from averagemeter import *\n",
    "from models import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import sampler\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import warnings\n",
    "\n",
    "INPUT_SIZE = 224\n",
    "NUM_CLASSES = 185\n",
    "NUM_EPOCHS = 30*3\n",
    "LEARNING_RATE = 1e-2\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "print(USE_CUDA)\n",
    "best_prec1 = 0\n",
    "classes = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecdbb307-f31b-4856-97b3-e089eafb03c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# This file provides a long_running decorator to indicate that a function needs a long amount of time to complete and\n",
    "# the computer should not enter standby. This file currently only works on Windows and is a no-op on other platforms.\n",
    "\n",
    "#copied from Kevin Barnes/kbarnes3: https://gist.github.com/kbarnes3/3fb7d353e9bdd3efccd5\n",
    "\n",
    "import ctypes\n",
    "import platform\n",
    "\n",
    "ES_CONTINUOUS = 0x80000000\n",
    "ES_SYSTEM_REQUIRED = 0x00000001\n",
    "\n",
    "\n",
    "def _set_thread_execution(state):\n",
    "    ctypes.windll.kernel32.SetThreadExecutionState(state)\n",
    "\n",
    "\n",
    "def prevent_standby():\n",
    "    if platform.system() == 'Windows':\n",
    "        _set_thread_execution(ES_CONTINUOUS | ES_SYSTEM_REQUIRED)\n",
    "\n",
    "\n",
    "def allow_standby():\n",
    "    if platform.system() == 'Windows':\n",
    "        _set_thread_execution(ES_CONTINUOUS)\n",
    "\n",
    "\n",
    "def long_running(func):\n",
    "    def inner(*args, **kwargs):\n",
    "        prevent_standby()\n",
    "        result = func(*args, **kwargs)\n",
    "        allow_standby()\n",
    "        return result\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92d00b18-18e1-4577-acc6-37459b13a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torchvision.models.vgg16(pretrained=True)\n",
    "# model = torchvision.models.vgg16(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f22d1774-4a20-4867-b34d-299b06c8e354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vjosv\\anaconda3\\envs\\cuda_env\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vjosv\\anaconda3\\envs\\cuda_env\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# model.eval()\n",
    "\n",
    "model = torchvision.models.vgg19(pretrained=True)\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = True\n",
    "model.classifier[-1]=nn.Linear(4096, 185)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adf713d-532f-4130-a6af-c1ed9403d8e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a89aee2-5215-4124-961a-3d246d101aea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# model = models.resnet101(weights=False)\n",
    "# model.fc = nn.Linear(2048, 185)\n",
    "# model.name='resnet101'\n",
    "\n",
    "# model = VGG('VGG19')\n",
    "model.name='vgg19_pretrained'\n",
    "if USE_CUDA:\n",
    "    model.cuda()\n",
    "# model = VGG('VGG16')\n",
    "# model.name='vgg16_new'\n",
    "# model = resnet101()\n",
    "# model = densenet121()\n",
    "# model.name='densenet121'\n",
    "\n",
    "\n",
    "\n",
    "traindir = os.path.join('dataset', 'train')\n",
    "testdir = os.path.join('dataset', 'test')\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "data_train = datasets.ImageFolder(traindir, transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize]))\n",
    "data_test = datasets.ImageFolder(testdir, transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize]))\n",
    "classes = data_train.classes\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=64, shuffle=True, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(data_test, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE,\n",
    "                      momentum=0.9, weight_decay=1e-4, nesterov=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "overload_training = False\n",
    "\n",
    "if os.path.isfile(f'{model.name}_checkpoint.pth.tar') and not overload_training:\n",
    "    checkpoint = torch.load(f'saved_models/{model.name}_checkpoint.pth.tar')\n",
    "    # checkpoint = torch.load(f'saved_models/{model.name}_model_best.pth.tar')\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "941674d2-3856-4bcb-8f2a-a51e869a7371",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # model.eval()\n",
    "if not os.path.isdir(f'saved_output/{model.name}_validating_output'):\n",
    "    os.mkdir(f'saved_output/{model.name}_validating_output')\n",
    "    \n",
    "#     (f'{model.name}_training_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5d701d5-dd28-40ba-991c-c2ac2a89471a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(data_test, batch_size=64, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d248fec-2b1f-4b0b-aba5-b91f72d59dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5190e80a-b70f-48b1-b329-c04b01f703e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_output_target(model,output,target,epoch,batch):\n",
    "        torch.save(output,f'saved_output/{model.name}_training_output/output_e{epoch}_b{batch}')\n",
    "        torch.save(target,f'saved_output/{model.name}_training_output/target_e{epoch}_b{batch}')\n",
    "    \n",
    "def save_output_target_validate(model,output,target,epoch,batch):\n",
    "        torch.save(output,f'saved_output/{model.name}_validating_output/output_e{epoch}_b{batch}')\n",
    "        torch.save(target,f'saved_output/{model.name}_validating_output/target_b{batch}')\n",
    "        \n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    if epoch == 9:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        if USE_CUDA:\n",
    "            input = input.cuda(non_blocking=True)\n",
    "            target = target.cuda(non_blocking=True)\n",
    "\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input_var = torch.autograd.Variable(input)\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        # print(output)\n",
    "\n",
    "        loss = criterion(output, target_var)\n",
    "        # print(loss)\n",
    "        # if i >10:\n",
    "        #     break\n",
    "        \n",
    "        # save_output_target(model,output,target_var,epoch,i)\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5,conf = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "        top5.update(prec5.item(), input.size(0))\n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  '\\Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                      data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    if (epoch-1) < 0:\n",
    "        former_lr=LEARNING_RATE\n",
    "    else:\n",
    "        former_lr = LEARNING_RATE * (0.1 ** ((epoch-1) // 30))\n",
    "    lr = LEARNING_RATE * (0.1 ** (epoch // 30))\n",
    "    if (lr <= 0.0001):\n",
    "        lr = 0.0001\n",
    "    print('\\n[Learning Rate] {:0.6f}'.format(lr))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr != former_lr\n",
    "\n",
    "# def accuracy(output, target, topk=(1,)):\n",
    "#     \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "#     maxk = max(topk)\n",
    "    \n",
    "#     batch_size = target.size(0)\n",
    "\n",
    "#     _, pred = output.topk(maxk, 1, True, True)\n",
    "#     pred = pred.t()\n",
    "\n",
    "#     correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    \n",
    "#     res = []\n",
    "#     for k in topk:\n",
    "#     \tcorrect_k = torch.sum(torch.reshape(correct[:k],(-1,) ).float()) #.double().sum(0)\n",
    "#     \tres.append(correct_k.mul_(100.0 / batch_size))\n",
    "#     return res\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename=f'saved_models/{model.name}_checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        print('\\n[INFO] Saved Model to model_best.pth.tar')\n",
    "        shutil.copyfile(filename, f'saved_models/{model.name}_model_best.pth.tar')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a283e142-9d5a-4122-8b4f-0b57a741dce4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    \n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred_t = pred.t()\n",
    "\n",
    "    correct = pred_t.eq(target.view(1, -1).expand_as(pred_t))\n",
    "\n",
    "    nb_classes = 185\n",
    "\n",
    "    confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "    _, preds = torch.max(output, 1)\n",
    "    \n",
    "    for t, p in zip(target.view(-1), preds.view(-1)):\n",
    "        confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "    # print(confusion_matrix)\n",
    "    \n",
    "    \n",
    "    # print(confusion_matrix.diag()/confusion_matrix.sum(1))\n",
    "        \n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = torch.sum(torch.reshape(correct[:k],(-1,) ).float()) #.double().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    res.append(confusion_matrix)\n",
    "    return res\n",
    "\n",
    "\n",
    "tt=''\n",
    "within_square = ''\n",
    "sample_pos =''\n",
    "output_aug_kde001 = ''\n",
    "weight_tensor_kde001 = ''\n",
    "zero_tensor_kde001=''\n",
    "def validate(val_loader, model, criterion,epoch,save_output=False):\n",
    "    global output_aug_kde001, sample_pos, within_square, weight_tensor_kde001,zero_tensor_kde001,tt\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # losses_aug =  AverageMeter()\n",
    "    # top1_aug = AverageMeter()\n",
    "    # top5_aug = AverageMeter()\n",
    "    # conf_aug = AverageMeter()\n",
    "    \n",
    "    # top1_aug500 = AverageMeter()\n",
    "    # top5_aug500 = AverageMeter()\n",
    "    # conf_aug500 = AverageMeter()\n",
    "    \n",
    "    # top1_aug_norm1_250sd = AverageMeter()\n",
    "    # top5_aug_norm1_250sd = AverageMeter()\n",
    "    # conf_aug_norm1_250sd = AverageMeter()\n",
    "    \n",
    "    # top1_aug_norm2_500sd = AverageMeter()\n",
    "    # top5_aug_norm2_500sd = AverageMeter()\n",
    "    # conf_aug_norm2_500sd = AverageMeter()\n",
    "    \n",
    "    # top1_aug_kde05 = AverageMeter()\n",
    "    # top5_aug_kde05 = AverageMeter()\n",
    "    # conf_aug_kde05 = AverageMeter()\n",
    "    \n",
    "    # top1_aug_kde1 = AverageMeter()\n",
    "    # top5_aug_kde1 = AverageMeter()\n",
    "    # conf_aug_kde1 = AverageMeter()\n",
    "    \n",
    "    conf = AverageMeter()\n",
    "    class_correct = list(0. for i in range(185))\n",
    "    class_total = list(0. for i in range(185))\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        if USE_CUDA:\n",
    "            input = input.cuda(non_blocking=True)\n",
    "            target = target.cuda(non_blocking=True)\n",
    "        input_var = torch.autograd.Variable(input, volatile=True)\n",
    "        target_var = torch.autograd.Variable(target, volatile=True)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        \n",
    "        # weight_tensor_1000 = np.ones((output.size(0),185))*0.1\n",
    "        # weight_tensor_500 = np.ones((output.size(0),185))*0.1\n",
    "        # weight_tensor_1500 = np.ones((output.size(0),185))*0.1\n",
    "\n",
    "\n",
    "        \n",
    "        # weight_tensor_normal250sd = np.ones((output.size(0),185))*stats_distance1.pdf(1000)\n",
    "        # weight_tensor_normal500sd = np.ones((output.size(0),185))*stats_distance2.pdf(1000)\n",
    "        # sample_pos_list = []\n",
    "        # for j in range(output.size(0)):\n",
    "        #     sample_pos = sample_plant_position(int(target[j]),dat)\n",
    "        #     sample_pos_list.append(sample_pos)\n",
    "        \n",
    "        # for sample_pos in sample_pos_list:\n",
    "        \n",
    "            # points_in_area_list = get_points_within(sample_pos)\n",
    "            # points_in_area = dat[dat['Id'].isin(points_in_area_list)]\n",
    "\n",
    "            # points_in_area_list_500 = get_points_within(sample_pos,distance=500)\n",
    "            # points_in_area_500 = dat[dat['Id'].isin(points_in_area_list_500)]\n",
    "            \n",
    "            \n",
    "            # indexes_in_area = [ names_mapping[i] for i in list(points_in_area['Vitenskapelig navn'].unique())]\n",
    "\n",
    "\n",
    "            \n",
    "            # for k in indexes_in_area:\n",
    "            #     weight_tensor_1000[j][k]=1\n",
    "\n",
    "            # indexes_in_area_500 = [ names_mapping[i] for i in list(points_in_area_500['Vitenskapelig navn'].unique())]\n",
    "\n",
    "            # for k in indexes_in_area_500:\n",
    "            #     weight_tensor_500[j][k]=1\n",
    "\n",
    "            \n",
    "            # id = int(sample_pos['Id'].iloc[0])\n",
    "            \n",
    "            # # print(sample_pos)\n",
    "            # # v = point_tree.query_ball_point([[[int(dat['Østkoordinat'].iloc[0])],int(dat['Nordkoordinat'].iloc[0])]], distance,return_sorted= True)[0]\n",
    "            # v = point_tree.query_ball_point([[int(sample_pos['Østkoordinat'].iloc[0]),int(sample_pos['Nordkoordinat'].iloc[0])]], 1000)[0]\n",
    " \n",
    "            \n",
    "            # return_dat = dat.iloc[v]\n",
    "\n",
    "            # return_dat = return_dat[return_dat.Id != sample_pos['Id'].iloc[0]]\n",
    "            \n",
    "            # points_in_area_list = list(return_dat['points'])\n",
    "            # # points_in_area_list.remove(id)\n",
    "        \n",
    "            \n",
    "            # # print(sample_pos.index[0])\n",
    "            # # print(sample_pos)\n",
    "            \n",
    "            # distance_list = distance_between_points(dat['points'].loc[sample_pos.index[0]],points_in_area_list)\n",
    "            # return_dat['distance'] = distance_list\n",
    "\n",
    "            # return_dat_sorted = return_dat.sort_values('distance')\n",
    "            \n",
    "            # return_dat_sorted['weight1'] = stats_distance1.pdf(return_dat_sorted['distance'])\n",
    "            # return_dat_sorted['weight2'] = stats_distance2.pdf(return_dat_sorted['distance'])\n",
    "            # return_dat_sorted['index'] = [names_mapping[i] for i in return_dat_sorted['Vitenskapelig navn']]\n",
    "            \n",
    "            \n",
    "\n",
    "            # for k,kk,kkk in zip(list(return_dat_sorted['index']),list(return_dat_sorted['weight1']),list(return_dat_sorted['weight2'])):\n",
    "            #     weight_tensor_normal250sd[j][k]=kk\n",
    "            #     weight_tensor_normal500sd[j][k]=kkk\n",
    "            \n",
    "    \n",
    "        # output_aug_kde05 = kde_augmentet_output(sample_pos_list,output, dat, 0.5)\n",
    "        # output_aug_kde1 = kde_augmentet_output(sample_pos_list,output, dat, 1)\n",
    "        # plants_in_area_tensor =torch.tensor(weight_tensor_1000)  \n",
    "\n",
    "        # plants_in_area_tensor500 =torch.tensor(weight_tensor_500)  \n",
    "        \n",
    "        # plants_in_area_tensor_norm250sd = torch.tensor(weight_tensor_normal250sd)\n",
    "        # plants_in_area_tensor_norm500sd = torch.tensor(weight_tensor_normal500sd)\n",
    "        \n",
    "\n",
    "        # output_aug = augment_output(output,plants_in_area_tensor)\n",
    "\n",
    "        # output_aug500 = augment_output(output,plants_in_area_tensor500)\n",
    "\n",
    "        \n",
    "        \n",
    "        # output_aug_norm250sd = augment_output(output,plants_in_area_tensor_norm250sd)\n",
    "        \n",
    "\n",
    "        # output_aug_norm500sd = augment_output(output,plants_in_area_tensor_norm500sd)\n",
    "        \n",
    "        \n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        save_output_target_validate(model,output,target,epoch,i)\n",
    "        \n",
    "        # loss_aug_crit = nn.NLLLoss()\n",
    "\n",
    "        # loss_aug = loss_aug_crit(output,target)\n",
    "        # measure accuracy and record loss\n",
    "        \n",
    "        prec1, prec5, conf1 = accuracy(output.data, target, topk=(1, 5))\n",
    "        \n",
    "        # prec1_aug, prec5_aug,conf_aug1 = accuracy(output_aug.data, target, topk=(1, 5))\n",
    "        # prec1_aug500, prec5_aug500,conf_500 = accuracy(output_aug500.data, target, topk=(1, 5))\n",
    "        # prec1_aug_norm1, prec5_aug_norm1,conf_augnorm1 = accuracy(output_aug_norm250sd.data, target, topk=(1, 5))\n",
    "        # prec1_aug_norm2, prec5_aug_norm2,conf_augnorm2 = accuracy(output_aug_norm500sd.data, target, topk=(1, 5))\n",
    "        # prec1_aug_kde05, prec5_aug_kde05,conf_kde05 = accuracy(output_aug_kde05.data, target, topk=(1, 5))\n",
    "        # prec1_aug_kde1, prec5_aug_kde1 , conf_kde1= accuracy(output_aug_kde1.data, target, topk=(1, 5))\n",
    "        \n",
    "        losses.update(loss.data.item(), input.size(0))\n",
    "        \n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "        top5.update(prec5.item(), input.size(0))\n",
    "\n",
    "        # losses_aug.update(loss_aug.data.item(), input.size(0))\n",
    "        # top1_aug.update(prec1_aug.item(), input.size(0))\n",
    "        # top5_aug.update(prec5_aug.item(), input.size(0))\n",
    "        # conf_aug.update(conf_aug1,1)\n",
    "        \n",
    "        # top1_aug500.update(prec1_aug500.item(), input.size(0))\n",
    "        # top5_aug500.update(prec5_aug500.item(), input.size(0))\n",
    "        # conf_aug500.update(conf_500,1)\n",
    "        \n",
    "        # top1_aug_norm1_250sd.update(prec1_aug_norm1.item(), input.size(0))\n",
    "        # top5_aug_norm1_250sd.update(prec5_aug_norm1.item(), input.size(0))\n",
    "        # conf_aug_norm1_250sd.update(conf_augnorm1,1)\n",
    "        \n",
    "        # top1_aug_norm2_500sd.update(prec1_aug_norm2.item(), input.size(0))\n",
    "        # top5_aug_norm2_500sd.update(prec5_aug_norm2.item(), input.size(0))\n",
    "        # conf_aug_norm2_500sd.update(conf_augnorm2,1)\n",
    "        \n",
    "        # top1_aug_kde05.update(prec1_aug_kde05.item(),input.size(0))\n",
    "        # top5_aug_kde05.update(prec5_aug_kde05.item(),input.size(0))\n",
    "        # conf_aug_kde05.update(conf_kde05,1)\n",
    "\n",
    "        # top1_aug_kde1.update(prec1_aug_kde1.item(),input.size(0))\n",
    "        # top5_aug_kde1.update(prec5_aug_kde1.item(),input.size(0))\n",
    "        # conf_aug_kde1.update(conf_kde1,1)\n",
    "        conf.update(conf1,1)\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 1 == 0:\n",
    "\n",
    "            print('Test: [{0}/{1}]\\n'.format(i, len(val_loader)))\n",
    "            print('Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\n'.format(batch_time=batch_time))\n",
    "            print('Loss {loss.val:.4f} ({loss.avg:.4f})\\n'.format(loss=losses))\n",
    "            print('Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\n'.format(top1=top1))\n",
    "            print('Prec@5 {top5.val:.3f} ({top5.avg:.3f})\\n'.format(top5=top5))\n",
    "            # print(conf.sum.diag()/conf.sum.sum(1))\n",
    "            # print('Loss_aug {loss_aug.val:.4f} {loss_aug.avg:.4f})\\n'.format(loss_aug=losses_aug))\n",
    "            # print('Prec@1_aug {top1_aug.val:.3f} ({top1_aug.avg:.3f})\\n'.format(top1_aug=top1_aug))\n",
    "            # print('Prec@5_aug {top5_aug.val:.3f} ({top5_aug.avg:.3f})\\n'.format(top5_aug=top5_aug))\n",
    "            # print(conf_aug.sum.diag()/conf_aug.sum.sum(1))\n",
    "            # print('Prec@1_aug500 {top1_aug500.val:.3f} ({top1_aug500.avg:.3f})\\n'.format(top1_aug500=top1_aug500))\n",
    "            # print('Prec@5_aug500 {top5_aug500.val:.3f} ({top5_aug500.avg:.3f})\\n'.format(top5_aug500=top5_aug500))\n",
    "            # print(conf_aug500.sum.diag()/conf_aug500.sum.sum(1))\n",
    "            # print('Prec@1_aug_norm1 {top1_aug_norm1_250sd.val:.3f} ({top1_aug_norm1_250sd.avg:.3f})\\n'.format(top1_aug_norm1_250sd =top1_aug_norm1_250sd))\n",
    "            # print('Prec@5_aug_norm1 {top5_aug_norm1.val:.3f} ({top5_aug_norm1.avg:.3f})\\n'.format(top5_aug_norm1=top5_aug_norm1_250sd))\n",
    "            # print(conf_aug_norm1_250sd.sum.diag()/conf_aug_norm1_250sd.sum.sum(1))\n",
    "            # print('Prec@1_aug_norm2 {top1_aug_norm2_500sd.val:.3f} ({top1_aug_norm2_500sd.avg:.3f})\\n'.format(top1_aug_norm2_500sd=top1_aug_norm2_500sd))\n",
    "            # print('Prec@5_aug_norm2 {top5_aug_norm2.val:.3f} ({top5_aug_norm2.avg:.3f})\\n'.format(top5_aug_norm2=top5_aug_norm2_500sd))\n",
    "            # print(conf_aug_norm2_500sd.sum.diag()/conf_aug_norm2_500sd.sum.sum(1))\n",
    "            # print('Prec@1_aug_kde05 {top1_aug_kde05.val:.3f} ({top1_aug_kde05.avg:.3f})\\n'.format(top1_aug_kde05=top1_aug_kde05))\n",
    "            # print('Prec@5_aug_kde05 {top5_aug_kde05.val:.3f} ({top5_aug_kde05.avg:.3f})\\n'.format(top5_aug_kde05=top5_aug_kde05))\n",
    "            # print(conf_aug_kde05.sum.diag()/conf_aug_kde05.sum.sum(1))\n",
    "            # print('Prec@1_aug_kde1 {top1_aug_kde1.val:.3f} ({top1_aug_kde1.avg:.3f})\\n'.format(top1_aug_kde1=top1_aug_kde1))\n",
    "            # print('Prec@5_aug_kde1 {top5_aug_kde1.val:.3f} ({top5_aug_kde1.avg:.3f})\\n'.format(top5_aug_kde1=top5_aug_kde1))\n",
    "            # print(conf_aug_kde1.sum.diag()/conf_aug_kde1.sum.sum(1))\n",
    "            # tt=conf_aug_kde1\n",
    "        print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n",
    "              .format(top1=top1, top5=top5))\n",
    "        \n",
    "\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "def augment_output(output, augment):\n",
    "    output_aug = torch.nn.functional.softmax(output, dim=1)\n",
    "    output_aug = output_aug * augment\n",
    "    output_aug = output_aug * 1/torch.sum(output_aug) \n",
    "    return torch.log(output_aug)\n",
    "\n",
    "def augment_output2(output, augment, zero_tensor_kde001):\n",
    "    output_aug = torch.nn.functional.softmax(output, dim=1)\n",
    "    augment = augment + zero_tensor_kde001*augment.min()\n",
    "    output_aug = output_aug + augment\n",
    "    output_aug = output_aug * 1/torch.sum(output_aug) \n",
    "    \n",
    "    return torch.log(output_aug)\n",
    "\n",
    "def kde_augmentet_output(sample_pos_list,output, dat, bandwidth):\n",
    "    \n",
    "    weight_tensor_kde = np.zeros((output.size(0),185))\n",
    "    zero_tensor_kde = np.ones((output.size(0),185))\n",
    "    for j, sample_pos in enumerate(sample_pos_list):\n",
    "        within_square =get_points_within_square(sample_pos,dat)\n",
    "        for plant_name in within_square.value_counts('Vitenskapelig navn').index:\n",
    "            within_square_ = within_square[within_square['Vitenskapelig navn']==plant_name]\n",
    "        \n",
    "            pj = names_mapping[plant_name]\n",
    "            pj_value = kernel_density_estimate_value(sample_pos,within_square_,bandwidth = bandwidth)\n",
    "            weight_tensor_kde[j][pj] = pj_value\n",
    "            zero_tensor_kde[j][pj] = 0\n",
    "    weight_tensor_kde = torch.tensor(weight_tensor_kde)\n",
    "    zero_tensor_kde = torch.tensor(zero_tensor_kde)\n",
    "    \n",
    "    output_aug_kde = augment_output2(output,weight_tensor_kde,zero_tensor_kde)\n",
    "\n",
    "    return output_aug_kde\n",
    "    \n",
    "stats_distance1 = norm(\n",
    "    loc=0, \n",
    "    scale=250\n",
    ")\n",
    "\n",
    "stats_distance2 = norm(\n",
    "    loc=0, \n",
    "    scale=500\n",
    ")\n",
    "# def plants_in_area(,x)\n",
    "#     more_than_x_obervations_of_target = False\n",
    "#     while not more_than_x_obervations_of_target:\n",
    "#         sample_plant_position()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aa066d-affa-42a7-b3b9-4d25912e2370",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d61c9bf-edf5-4674-bca9-9be5b0b70ce6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint 'saved_models/vgg19_pretrained_checkpoint.pth.tar'\n",
      "=> loaded checkpoint 'saved_models/vgg19_pretrained_checkpoint.pth.tar' (epoch 45)\n",
      "\n",
      "[INFO] Training Started\n",
      "\n",
      "[Learning Rate] 0.001000\n",
      "Epoch: [45][0/1475]\t\\Time 4.567 (4.567)\tData 4.229 (4.229)\tLoss 0.0121 (0.0121)\tPrec@1 98.438 (98.438)\tPrec@5 100.000 (100.000)\n",
      "Epoch: [45][100/1475]\t\\Time 0.123 (0.168)\tData 0.088 (0.128)\tLoss 0.0199 (0.0141)\tPrec@1 100.000 (99.134)\tPrec@5 100.000 (100.000)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 79\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[INFO] Saved Model to leafsnap_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;66;03m# torch.save(model, f'{model.name}_checkpoint.pth.tar')\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m train_model(model)\n",
      "Cell \u001b[1;32mIn[2], line 30\u001b[0m, in \u001b[0;36mlong_running.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     29\u001b[0m     prevent_standby()\n\u001b[1;32m---> 30\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     31\u001b[0m     allow_standby()\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "Cell \u001b[1;32mIn[11], line 64\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, LEARNING_RATE, NUM_EPOCHS)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=> loaded checkpoint \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (epoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     61\u001b[0m           \u001b[38;5;241m.\u001b[39mformat(args_resume, checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# train for one epoch\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m train(train_loader, model, criterion, optimizer, epoch)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# evaluate on validation set\u001b[39;00m\n\u001b[0;32m     66\u001b[0m prec1 \u001b[38;5;241m=\u001b[39m validate(val_loader, model, criterion,epoch,save_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[8], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (\u001b[38;5;28minput\u001b[39m, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# measure data loading time\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_CUDA:\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mcuda(non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     28\u001b[0m         target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mcuda(non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     30\u001b[0m     data_time\u001b[38;5;241m.\u001b[39mupdate(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m end)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "# args_resume = f'{model.name}_model_best.pth.tar'\n",
    "args_resume = f'saved_models/{model.name}_checkpoint.pth.tar'\n",
    "# if args_resume:\n",
    "#     if os.path.isfile(args_resume):\n",
    "#         print(\"=> loading checkpoint '{}'\".format(args_resume))\n",
    "#         checkpoint = torch.load(args_resume)\n",
    "#         start_epoch = checkpoint['epoch']\n",
    "#         best_prec1 = checkpoint['best_prec1']\n",
    "#         model.load_state_dict(checkpoint['state_dict'])\n",
    "#         optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "#         print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "#               .format(args_resume, checkpoint['epoch']))\n",
    "#         start_epoch = checkpoint['epoch']\n",
    "#     else:\n",
    "#         print(\"=> no checkpoint found at '{}'\".format(args_resume))\n",
    "\n",
    "# print('\\n[INFO] Training Started')\n",
    "\n",
    "@long_running\n",
    "def train_model(model,LEARNING_RATE = LEARNING_RATE,NUM_EPOCHS = NUM_EPOCHS):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE,\n",
    "                      momentum=0.9, weight_decay=1e-4, nesterov=True)\n",
    "    train_loader = torch.utils.data.DataLoader(data_train, batch_size=64, shuffle=True, num_workers=2)\n",
    "    val_loader = torch.utils.data.DataLoader(data_test, batch_size=64, shuffle=False, num_workers=2)\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE,\n",
    "                          momentum=0.9, weight_decay=1e-4, nesterov=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    args_resume = f'saved_models/{model.name}_checkpoint.pth.tar'\n",
    "    if args_resume:\n",
    "        if os.path.isfile(args_resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(args_resume))\n",
    "            checkpoint = torch.load(args_resume)\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            best_prec1 = checkpoint['best_prec1']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(args_resume, checkpoint['epoch']))\n",
    "            start_epoch = checkpoint['epoch']\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args_resume))\n",
    "\n",
    "            start_epoch = 0\n",
    "            best_prec1 = 0\n",
    "    print('\\n[INFO] Training Started')\n",
    "\n",
    "    for epoch in range(start_epoch, NUM_EPOCHS ):\n",
    "        adjusted_rate = adjust_learning_rate(optimizer, epoch)\n",
    "        \n",
    "        if adjusted_rate:\n",
    "            args_resume = f'saved_models/{model.name}_model_best.pth.tar'\n",
    "            print(\"=> loading checkpoint '{}'\".format(args_resume))\n",
    "            checkpoint = torch.load(args_resume)\n",
    "            best_prec1 = checkpoint['best_prec1']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(args_resume, checkpoint['epoch']))\n",
    "\n",
    "        # train for one epoch\n",
    "        train(train_loader, model, criterion, optimizer, epoch)\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate(val_loader, model, criterion,epoch,save_output=True)\n",
    "\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "        print('\\n[INFO] Saved Model to leafsnap_model.pth')\n",
    "        # torch.save(model, f'{model.name}_checkpoint.pth.tar')\n",
    "        \n",
    "train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d30e870-2650-483e-a24e-c244d3b2ce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bfc5c7-3bb7-4591-a6d8-affb5abf93da",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69732d2e-cd31-4518-9a99-325a1d729ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "args_resume = f'saved_models/{model.name}_checkpoint.pth.tar'\n",
    "if args_resume:\n",
    "    if os.path.isfile(args_resume):\n",
    "        print(\"=> loading checkpoint '{}'\".format(args_resume))\n",
    "        checkpoint = torch.load(args_resume)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        best_prec1 = checkpoint['best_prec1']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "              .format(args_resume, checkpoint['epoch']))\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        NUM_EPOCHS = checkpoint['NUM_EPOCHS']\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(args_resume))\n",
    "\n",
    "        start_epoch = 0\n",
    "        best_prec1 = 0\n",
    "\n",
    "validate(test_loader,model,criterion,epoch)\n",
    "\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57162fde-7675-4a2b-af91-e77892c3638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat['points'][727362]\n",
    "\n",
    "\n",
    "# dat.iloc[727362]\n",
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fcbea13-dd6a-43b2-9a7e-c2eb67557f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vjosv\\AppData\\Local\\Temp\\ipykernel_17404\\3690604550.py:22: DtypeWarning: Columns (14,15,24,25,26,37,38,39,40,46) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dat = pd.read_csv('C:/Users/vjosv/master/dataset/top185_in_oslo_area.csv')\n",
      "C:\\Users\\vjosv\\AppData\\Local\\Temp\\ipykernel_17404\\3690604550.py:17: FutureWarning: This function is deprecated. See: https://pyproj4.github.io/pyproj/stable/gotchas.html#upgrading-to-pyproj-2-from-pyproj-1\n",
      "  lon, lat = transform(utm_proj, lonlat_proj, df['Østkoordinat'].values, df['Nordkoordinat'].values)\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import scipy.spatial as spatial\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "import pandas as pd\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from sklearn.datasets import fetch_species_distributions\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import copy\n",
    "from pyproj import Proj, transform\n",
    "\n",
    "def convert_utm_to_latlon(df, zone_number, zone_letter):\n",
    "    utm_proj = Proj(proj='utm', zone=zone_number, ellps='WGS84', south=(zone_letter < 'N'))\n",
    "    lonlat_proj = Proj(proj='latlong', datum='WGS84')\n",
    "    lon, lat = transform(utm_proj, lonlat_proj, df['Østkoordinat'].values, df['Nordkoordinat'].values)\n",
    "    \n",
    "    return pd.DataFrame({'Longitude': lon, 'Latitude': lat})\n",
    "\n",
    "\n",
    "dat = pd.read_csv('C:/Users/vjosv/master/dataset/top185_in_oslo_area.csv')\n",
    "dat = dat[['Id','Østkoordinat','Nordkoordinat','Vitenskapelig navn']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dat['points']= [[i,j] for i, j in zip(dat.Østkoordinat, dat.Nordkoordinat)]\n",
    "\n",
    "df_latlon = convert_utm_to_latlon(dat, 33, 'N') \n",
    "\n",
    "\n",
    "dat['lat']=df_latlon['Latitude']\n",
    "dat['long']=df_latlon['Longitude']\n",
    "\n",
    "\n",
    "\n",
    "#oslo area:\n",
    "#lowerleft, upperleft, upper right, lower right,\n",
    "\n",
    "\n",
    "lat_long_oslo = [(58.998141, 9.574585), (60.351413, 9.574585), (60.351413, 12.540894),(58.998141,12.540894)]\n",
    "\n",
    "# dat['Østkoordinat']>lat_long_oslo[0][0]\n",
    "\n",
    "\n",
    "dat = dat.loc[(dat['lat']>lat_long_oslo[0][0])  & (dat['lat']<lat_long_oslo[1][0]) & (dat['long']>lat_long_oslo[0][1 ]) & (dat['long']<lat_long_oslo[2][1])]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "points = dat[['Østkoordinat','Nordkoordinat']].to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "point_tree = spatial.cKDTree(points)\n",
    "\n",
    "\n",
    "science_names = dat['Vitenskapelig navn'].unique()\n",
    "indexes = [i for i in range(len(dat['Vitenskapelig navn'].unique()))]\n",
    "\n",
    "\n",
    "random.seed(10)\n",
    "\n",
    "random.shuffle(indexes)\n",
    "\n",
    "\n",
    "names_mapping = {science_name : index for (science_name,index) in zip(science_names,indexes)}\n",
    "index_mapping = {index : science_name for (science_name,index) in zip(science_names,indexes)}\n",
    "\n",
    "def distance_between_points(point, list_of_points):\n",
    "    return [np.sqrt(np.power(point[0]-lop[0],2)+np.power(point[1]-lop[1],2)) for lop in list_of_points]\n",
    "\n",
    "\n",
    "\n",
    "def get_points_within(df_row, distance=1000):\n",
    "    id = int(df_row['Id'].iloc[0])\n",
    "    return_list = point_tree.query_ball_point([[int(df_row['Østkoordinat'].iloc[0]),int(df_row['Nordkoordinat'].iloc[0])]], distance)[0]\n",
    "    return_dat = dat.iloc[return_list]\n",
    "    return_list = list(return_dat['Id'])\n",
    "    return_list.remove(id)\n",
    "    return return_list\n",
    "    \n",
    "def sample_plant_position(plant,df):\n",
    "    if type(plant) == str:\n",
    "        return dat[dat['Vitenskapelig navn'] == plant].sample(1)\n",
    "    elif type(plant) == int:\n",
    "        return dat[dat['Vitenskapelig navn'] == index_mapping[plant]].sample(1)\n",
    "\n",
    "def kernel_density_estimate_value(point_row,dat,bandwidth = 0.5):\n",
    "    # print(point_row)\n",
    "    # print(dat)\n",
    "    if point_row.index[0] in list(dat.index):\n",
    "        np_dat_lat_long = dat.drop(point_row.index[0])[['lat','long']].to_numpy()\n",
    "    else:\n",
    "        np_dat_lat_long = dat[['lat','long']].to_numpy()\n",
    "    kde = KernelDensity(bandwidth=bandwidth)\n",
    "    # print(np_dat_lat_long)\n",
    "    if len(np_dat_lat_long)==0:\n",
    "        return 0\n",
    "    kde.fit(np_dat_lat_long)\n",
    "    np_point = np.array([[point_row['lat'].iloc[0],point_row['long'].iloc[0]]])\n",
    "    return np.exp(kde.score_samples(np_point))[0]\n",
    "\n",
    "def get_points_within_square(point, dat,side_length = 3000):\n",
    "    return_dat = dat[dat['Østkoordinat']>point['Østkoordinat'].iloc[0]-side_length]\n",
    "    return_dat = return_dat[return_dat['Østkoordinat']<point['Østkoordinat'].iloc[0]+ side_length]\n",
    "    return_dat = return_dat[return_dat['Nordkoordinat']>point['Nordkoordinat'].iloc[0]- side_length]\n",
    "    return_dat = return_dat[return_dat['Nordkoordinat']<point['Nordkoordinat'].iloc[0]+ side_length]\n",
    "    return return_dat\n",
    "\n",
    "# within_square =get_points_within_square(v,dat)\n",
    "\n",
    "# within_square\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_points_within(df_row, distance=1000):\n",
    "    id = int(df_row['Id'].iloc[0])\n",
    "    return_list = point_tree.query_ball_point([[int(df_row['Østkoordinat'].iloc[0]),int(df_row['Nordkoordinat'].iloc[0])]], distance)[0]\n",
    "    return_dat = dat.iloc[return_list]\n",
    "    return_list = list(return_dat['Id'])\n",
    "    return_list.remove(id)\n",
    "    return return_list\n",
    "    \n",
    "def sample_plant_position(plant,df):\n",
    "    if type(plant) == str:\n",
    "        return dat[dat['Vitenskapelig navn'] == plant].sample(1)\n",
    "    elif type(plant) == int:\n",
    "        return dat[dat['Vitenskapelig navn'] == index_mapping[plant]].sample(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_knn_classifier(samples_pos_list,dat,n=1000):\n",
    "    sample_pos_indexes = [sp.index[0] for sp in samples_pos_list if sp.index[0] in list(dat.index)]\n",
    "    \n",
    "    dat_removed_samples = dat.drop(index=sample_pos_indexes)\n",
    "    category = []\n",
    "    for k in dat_removed_samples['Vitenskapelig navn']:\n",
    "        category.append(names_mapping[k])\n",
    "        \n",
    "    category = np.array(category)\n",
    "    points_np= np.array(list(dat_removed_samples['points']))\n",
    "    oversample = imblearn.over_sampling.KMeansSMOTE()\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    X, y = oversample.fit_resample(points_np, category) \n",
    "    warnings.filterwarnings(\"default\")\n",
    "    \n",
    "    neigh = KNeighborsClassifier(n_neighbors=n)\n",
    "    neigh.fit(X, y)\n",
    "    return neigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4252dcc-f9a2-48f1-a15c-5146d3c36960",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(dat['points'][0])\n",
    "names_mapping['Convallaria majalis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ecc083c-5219-4ea4-b7f6-77257ee98e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from geopy import distance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ec7d1e-bdb9-4799-985a-06d6b1a17a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_pos_list= []\n",
    "\n",
    "for i in [0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1]:\n",
    "    samples_pos_list.append(sample_plant_position(i,dat))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1d5086-ceb1-42a5-9d2f-f748ef5206ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a38f0f6-b539-4a6a-bd04-c6c4c5591360",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "knn = get_knn_classifier(samples_pos_list,dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e174a159-96f4-422f-9dd7-5f311d309814",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8642b02-5a7d-43ac-bbd4-ccef43fe5662",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in aa:\n",
    "    # print(i)\n",
    "    if i ==3:\n",
    "        print('ssssssssssssssss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8125f72-2b32-4b10-b411-b7d65afe1e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(sample_pos_list[-5]['points'])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e2621d-82ba-4de0-85ef-d2e068262440",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.predict_proba(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1967c5cf-26af-4a6e-a5bd-5e48ba5c83ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "category\n",
    "\n",
    "points_np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699dbf82-badf-4041-8456-7bb0f4bd60de",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=100)\n",
    "neigh.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f931d4-2ad9-4c78-b58a-b738a73903a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4505f3e5-f581-4b0c-b06e-41d09084637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "validate(test_loader,model,criterion)\n",
    "\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4058c6ba-a7f6-4ad9-b40f-0b993c2f796b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_var)\n",
    "\n",
    "\n",
    "loss = criterion(output, target_var)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb9e261-d406-489d-bcb1-d8dcfd39f05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(science_names)\n",
    "\n",
    "len(dat['Vitenskapelig navn'].unique())\n",
    "\n",
    "if 5 in range(5):\n",
    "    print('s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3dc5b4-da48-4eba-acd3-f59fb0913141",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467d8ba9-2603-4dbd-afcb-410e0e835954",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = int(dat['Id'].iloc[0])\n",
    "\n",
    "n = 10\n",
    "df_row = dat\n",
    "distance = 1000\n",
    "\n",
    "\n",
    "v = point_tree.query_ball_point([[int(df_row['Østkoordinat'].iloc[n]),int(df_row['Nordkoordinat'].iloc[n])]], distance,return_sorted= True)[0]\n",
    "\n",
    "\n",
    "return_dat = dat.iloc[v]\n",
    "return_list = list(return_dat['points'])\n",
    "def distance_between_points(point, list_of_points):\n",
    "    return [np.sqrt(np.power(point[0]-lop[0],2)+np.power(point[1]-lop[1],2)) for lop in list_of_points]\n",
    "\n",
    "\n",
    "\n",
    "distance_list = distance_between_points(dat['points'].iloc[n],return_list)\n",
    "\n",
    "return_dat['distance'] = distance_list\n",
    "\n",
    "return_dat_sorted = return_dat.sort_values('distance')\n",
    "return_dat_sorted\n",
    "\n",
    "\n",
    "indexes_in_area = [ names_mapping[i] for i in list(return_dat_sorted['Vitenskapelig navn'].unique())]\n",
    "\n",
    "indexes_in_area\n",
    "\n",
    "return_dat_sorted['Vitenskapelig navn'].unique()\n",
    "\n",
    "return_dat_sorted\n",
    "\n",
    "\n",
    "stats_distance0 = norm(\n",
    "    loc=0, \n",
    "    scale=1000\n",
    ")\n",
    "\n",
    "stats_distance1 = norm(\n",
    "    loc=0, \n",
    "    scale=500\n",
    ")\n",
    "\n",
    "stats_distance1.pdf(return_dat_sorted['distance'])\n",
    "\n",
    "return_dat_sorted['weight'] = stats_distance1.pdf(return_dat_sorted['distance'])\n",
    "return_dat_sorted['index'] = [names_mapping[i] for i in return_dat_sorted['Vitenskapelig navn']]\n",
    "\n",
    "\n",
    "return_dat_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15ace20-1352-4781-a953-d1ed3d2f6637",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(185))\n",
    "class_total = list(0. for i in range(185))\n",
    "\n",
    "# switch to evaluate mode\n",
    "model.eval()\n",
    "end = time.time()\n",
    "for i, (input, target) in enumerate(val_loader):\n",
    "    if USE_CUDA:\n",
    "        input = input.cuda(non_blocking=True)\n",
    "        target = target.cuda(non_blocking=True)\n",
    "    input_var = torch.autograd.Variable(input, volatile=True)\n",
    "    target_var = torch.autograd.Variable(target, volatile=True)\n",
    "\n",
    "    # compute output\n",
    "    output = model(input_var)\n",
    "    \n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9e88f5-0cef-471b-9d67-daf48c38301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output\n",
    "batch_size = target.size(0)\n",
    "\n",
    "batch_size\n",
    "\n",
    "_, pred = output.topk(5, 1, True, True)\n",
    "\n",
    "predt = pred.t()\n",
    "print(target.view(1, -1).expand_as(predt))\n",
    "print(predt)\n",
    "\n",
    "print(target)\n",
    "\n",
    "print(predt.eq(target.view(1, -1).expand_as(predt)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c0363e-96a2-431f-98a3-08f28dc662f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.topk(5, 1, True, True)\n",
    "\n",
    "pred.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357fb506-2025-4e69-91d7-7cfbd0b9e353",
   "metadata": {},
   "outputs": [],
   "source": [
    "    maxk = max(topk)\n",
    "    \n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    \n",
    "    res = []\n",
    "    for k in topk:\n",
    "    \tcorrect_k = torch.sum(torch.reshape(correct[:k],(-1,) ).float()) #.double().sum(0)\n",
    "    \tres.append(correct_k.mul_(100.0 / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a7dfe2-aa79-4c69-9315-c1a6da5781dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kernel_density_estimate_value(v,points_in_square_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78c5fd9-1575-4043-8b89-6b435e357a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_aug = nn.NLLLoss(output,target)\n",
    "\n",
    "\n",
    "loss_aug_crit = nn.NLLLoss()\n",
    "\n",
    "loss_aug = loss_aug_crit(output,target)\n",
    "loss_aug.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26a8add-03ad-48a1-b668-19425f7fa494",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.topk(5, 1, True, True))\n",
    "\n",
    "print(out.topk(5, 1, True, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63d0044-27dc-4178-8187-4fc794910378",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(output, dim=1)\n",
    "probs.topk(5, 1, True, True)\n",
    "\n",
    "print(output)\n",
    "print(torch.log(probs))\n",
    "torch.nn.functional.log_softmax(output)\n",
    "\n",
    "nn.NLLLoss(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f109013-2b9b-401a-af4c-7831938a5841",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(target_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b89079-1f67-4a31-9801-bb460e5ca164",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "name_to_index ={}\n",
    "for image in data_train.imgs:\n",
    "    name_to_index[image[0].split('/')[3]]=image[1]\n",
    "    \n",
    "index_to_name = [i[0] for i in name_to_index.items()] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747970c8-31d6-42c5-99d8-914289a96812",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42d5051-09c4-49b0-ac52-e8c61231c78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_to_index\n",
    "\n",
    "test_tensor = np.ones((output.size(0),185))*0.1\n",
    "\n",
    "test_tensor\n",
    "\n",
    "for i in test_tensor:\n",
    "    for j in i:\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeedb264-57a8-463c-bf0f-b0a7012154b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec21cf10-7303-4bfa-9074-2362d3436a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8712aa8-464f-4b7d-b264-a75147084cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pykrige\n",
    "# !pip install skgstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093d8473-1418-45b8-a3cd-5d6d750b7693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pykrige.kriging_tools as kt\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737fa265-d0d5-48f5-91b7-1de61dffadaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import scipy.spatial as spatial\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "import pandas as pd\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from sklearn.datasets import fetch_species_distributions\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import copy\n",
    "from pyproj import Proj, transform\n",
    "\n",
    "dat = pd.read_csv('dataset/lokasjon_top_185_with_height.csv')\n",
    "dat = dat[['Id','Østkoordinat','Nordkoordinat','Vitenskapelig navn']]\n",
    "\n",
    "\n",
    "\n",
    "dat['points'] = list(zip(dat.Østkoordinat, dat.Nordkoordinat))\n",
    "\n",
    "points = dat[['Østkoordinat','Nordkoordinat']].to_numpy()\n",
    "\n",
    "point_tree = spatial.cKDTree(points)\n",
    "\n",
    "\n",
    "science_names = dat['Vitenskapelig navn'].unique()\n",
    "indexes = [i for i in range(185)]\n",
    "random.seed(10)\n",
    "\n",
    "random.shuffle(indexes)\n",
    "\n",
    "\n",
    "names_mapping = {science_name : index for (science_name,index) in zip(science_names,indexes)}\n",
    "index_mapping = {index : science_name for (science_name,index) in zip(science_names,indexes)}\n",
    "\n",
    "\n",
    "def get_points_within(df_row, distance=1000):\n",
    "    id = int(df_row['Id'].iloc[0])\n",
    "    return_list = point_tree.query_ball_point([[int(df_row['Østkoordinat'].iloc[0]),int(df_row['Nordkoordinat'].iloc[0])]], distance)[0]\n",
    "    return_dat = dat.iloc[return_list]\n",
    "    return_list = list(return_dat['Id'])\n",
    "    return_list.remove(id)\n",
    "    return return_list\n",
    "    \n",
    "def sample_plant_position(plant,df):\n",
    "    if type(plant) == str:\n",
    "        return dat[dat['Vitenskapelig navn'] == plant].sample(1)\n",
    "    elif type(plant) == int:\n",
    "        return dat[dat['Vitenskapelig navn'] == index_mapping[plant]].sample(1)\n",
    "\n",
    "\n",
    "def convert_utm_to_latlon(df, zone_number, zone_letter):\n",
    "    utm_proj = Proj(proj='utm', zone=zone_number, ellps='WGS84', south=(zone_letter < 'N'))\n",
    "    lonlat_proj = Proj(proj='latlong', datum='WGS84')\n",
    "    lon, lat = transform(utm_proj, lonlat_proj, df['Østkoordinat'].values, df['Nordkoordinat'].values)\n",
    "    \n",
    "    return pd.DataFrame({'Longitude': lon, 'Latitude': lat})\n",
    "\n",
    "\n",
    "\n",
    "df_latlon = convert_utm_to_latlon(dat, 33, 'N') \n",
    "\n",
    "\n",
    "dat['lat']=df_latlon['Latitude']\n",
    "dat['long']=df_latlon['Longitude']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c7ca00-40dd-410d-a5f6-166d9a84415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = sample_plant_position(0,dat)\n",
    "vv = get_points_within(v)\n",
    "points_in_area = dat[dat['Id'].isin(vv)]\n",
    "\n",
    "\n",
    "indexes_in_area = [ names_mapping[i] for i in list(points_in_area['Vitenskapelig navn'].unique())]\n",
    "\n",
    "\n",
    "test_tensor = np.ones(185)*0.1\n",
    "for i in indexes_in_area:\n",
    "    test_tensor[i]=1\n",
    "\n",
    "\n",
    "\n",
    "# out = output*tt\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8e4582-e6ee-47ff-b236-345303b36ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13069ca9-c70a-4366-ac3b-29f82fa63be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def utm33_to_latlon(east,north):\n",
    "    utm_proj = Proj(proj='utm', zone=33, ellps='WGS84', south=('N' < 'N'))\n",
    "    lonlat_proj = Proj(proj='latlong', datum='WGS84')\n",
    "    lon, lat = transform(utm_proj, lonlat_proj, east, north)\n",
    "    return lat,lon\n",
    "\n",
    "def get_basemap(df_row, side_length= 3000):\n",
    "    lower_lat, lower_lon = utm33_to_latlon(df_row['Østkoordinat'].iloc[0]-side_length/2,df_row['Nordkoordinat'].iloc[0]-side_length/2)\n",
    "    print(lower_lat,lower_lon)\n",
    "    upper_lat, upper_lon = utm33_to_latlon(df_row['Østkoordinat'].iloc[0]+side_length/2,df_row['Nordkoordinat'].iloc[0]+side_length/2)\n",
    "    \n",
    "    map = Basemap(llcrnrlon=lower_lon,llcrnrlat=lower_lat,urcrnrlon=upper_lon,urcrnrlat=upper_lat,\n",
    "                 resolution='i', lat_0 = 39.5, lon_0 = -3.25)\n",
    "\n",
    "\n",
    "    print(lower_lon,upper_lon)\n",
    "    xgrid = np.arange(lower_lon,upper_lon,0.00001)\n",
    "\n",
    "    y0 = lower_lat\n",
    "    y1 = upper_lat\n",
    "    \n",
    "    ygrid = np.arange(y0,y1,(y1-y0)/len(xgrid))\n",
    "    return map, xgrid, ygrid\n",
    "    \n",
    "m, xgrid, ygrid = get_basemap(v)\n",
    "\n",
    "points_in_square = get_points_within_square(v,dat)\n",
    "\n",
    "latlong_np = points_in_square[['lat','long']].to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(10, 1)\n",
    "top_10_plants = list(points_in_square['Vitenskapelig navn'].value_counts()[:10].index)\n",
    "\n",
    "fig.subplots_adjust(left=0.05, right=0.95, wspace=0.05)\n",
    "# species_names = ['Bradypus Variegatus', 'Microryzomys Minutus']\n",
    "cmaps = ['Purples', 'Reds']\n",
    "for i, axi in enumerate(ax):\n",
    "    axi.set_title(top_10_plants[i])\n",
    "    mi= copy.deepcopy(m)\n",
    "    \n",
    "    points_in_square_ = points_in_square[points_in_square['Vitenskapelig navn']==top_10_plants[i]]\n",
    "\n",
    "    \n",
    "    ll = points_in_square_[['lat','long']].to_numpy()\n",
    "    \n",
    "    kde = KernelDensity(bandwidth=0.01)\n",
    "    kde.fit(ll)\n",
    "    \n",
    "    mi.fillcontinents(color='white')\n",
    "\n",
    "    mi.drawcoastlines()\n",
    "    mi.drawcountries()\n",
    "    # m.scatter(points_in_square['long'],points_in_square['lat'], zorder=3, cmap='rainbow', latlon=True);\n",
    "    \n",
    "    X, Y = np.meshgrid(xgrid[::5], ygrid[::5][::-1])\n",
    "    xy = np.vstack([Y.ravel(), X.ravel()]).T\n",
    "    \n",
    "    Z = np.exp(kde.score_samples(xy))\n",
    "    Z = Z.reshape(X.shape)\n",
    "    Z[0]=1\n",
    "    \n",
    "    # plot contours of the density\n",
    "    levels = np.linspace(0, Z.max(), 25)\n",
    "    axi.contourf(X, Y, Z, levels=levels, cmap='Reds')\n",
    "    vv = np.array([[v['lat'].iloc[0],v['long'].iloc[0]]])\n",
    "              \n",
    "    print(np.exp(kde.score_samples(vv)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35a3392-1e27-4b63-9fd1-7211313e0157",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "v\n",
    "\n",
    "vv = np.array([[v['lat'].iloc[0],v['long'].iloc[0]]])\n",
    "              \n",
    "np.exp(kde.score_samples(vv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216514ad-b375-4ce9-99a8-1d7a8642e812",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b0a996-85c7-4e31-a2be-90b5ee9d00ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "within_square.groupby('Vitenskapelig navn').size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb112c4-0b54-4c6c-8b7a-964cd18ddcd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d3c558-c62a-48a0-8a7c-51d3fdf0dc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "within_square.groupby('Vitenskapelig navn').size()\n",
    "\n",
    "a =within_square[within_square['Vitenskapelig navn'] == 'Aegopodium podagraria']\n",
    "\n",
    "H, xedges, yedges = np.histogram2d(a['Østkoordinat'], a['Nordkoordinat'], bins=10, range=None, density=None, weights=None)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(7, 3))\n",
    "ax = fig.add_subplot(131, title='imshow: square bins')\n",
    "plt.imshow(H, interpolation='nearest', origin='lower',\n",
    "        extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e41e84-e485-4a96-949e-14533f8e1f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "within_square.groupby('Vitenskapelig navn').size()\n",
    "\n",
    "a =within_square[within_square['Vitenskapelig navn'] == 'Acer platanoides']\n",
    "\n",
    "H, xedges, yedges = np.histogram2d(a['Østkoordinat'], a['Nordkoordinat'], bins=10, range=None, density=None, weights=None)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(7, 3))\n",
    "ax = fig.add_subplot(131, title='imshow: square bins')\n",
    "plt.imshow(H, interpolation='nearest', origin='lower',\n",
    "        extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8619818-277f-44d0-9a40-12842bf58d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "within_square.groupby('Vitenskapelig navn').size()\n",
    "\n",
    "a =within_square[within_square['Vitenskapelig navn'] == 'Vicia cracca']\n",
    "\n",
    "H, xedges, yedges = np.histogram2d(a['Østkoordinat'], a['Nordkoordinat'], bins=10, range=None, density=None, weights=None)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(7, 3))\n",
    "ax = fig.add_subplot(131, title='imshow: square bins')\n",
    "plt.imshow(H, interpolation='nearest', origin='lower',\n",
    "        extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fac1ed4-cd0a-4054-a327-8a63bb03a27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(H[0])\n",
    "# H\n",
    "# print(x,y)\n",
    "len(xedges)\n",
    "xpoints = []\n",
    "ypoints =[]\n",
    "for i in range(len(xedges)-1):\n",
    "    xpoints.append((xedges[i]+xedges[i+1])/2)\n",
    "    ypoints.append((yedges[i]+yedges[i+1])/2)\n",
    "xpoints\n",
    "ypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7952d80f-f2de-450f-a124-e01b1053b365",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# H =H.T\n",
    "# xedges\n",
    "# yedges\n",
    "\n",
    "\n",
    "x=[]\n",
    "y=[]\n",
    "yr=[]\n",
    "phi = []\n",
    "for i,ii in enumerate(H):\n",
    "    for j,jj in enumerate(ii):\n",
    "        x.append(xpoints[j])\n",
    "        y.append(ypoints[i])\n",
    "        yr.append(ypoints[i])\n",
    "        if jj != 0:\n",
    "            phi.append(jj*5)\n",
    "        else:\n",
    "            phi.append(0)\n",
    "        \n",
    "\n",
    "yr.reverse()\n",
    "\n",
    "\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "yr = np.array(yr)\n",
    "\n",
    "phi = np.array(phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a82b57a-7594-4ad3-9761-8d1b0af33ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sca = plt.scatter(x,y,c=phi)\n",
    "\n",
    "\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "print(phi)\n",
    "\n",
    "\n",
    "\n",
    "OK = OrdinaryKriging(x,yr,phi, variogram_model='exponential',verbose = True, enable_plotting=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60c0d0b-360d-41f5-b7a4-ee601b5a2866",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.arange(xedges[0],xedges[-1],10)\n",
    "yy = np.arange(yedges[0],yedges[-1],10)\n",
    "zstar, ss = OK.execute(\"grid\",xx,yy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8baa418-1d71-4f74-8338-d75ce3a1b5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = plt.imshow(zstar,extent=(xx[0],xx[-1],yy[0],yy[-1]))\n",
    "# plt.scatter(x,y,c=phi,marker='.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d5f1f5-d724-47ce-97a7-212cd6db77e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "def measure(n):\n",
    "    \"Measurement model, return two coupled measurements.\"\n",
    "    m1 = np.random.normal(size=n)\n",
    "    m2 = np.random.normal(scale=0.5, size=n)\n",
    "    return m1+m2, m1-m2\n",
    "    \n",
    "m1, m2 = measure(2000)\n",
    "xmin = m1.min()\n",
    "xmax = m1.max()\n",
    "ymin = m2.min()\n",
    "ymax = m2.max()\n",
    "X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "positions = np.vstack([X.ravel(), Y.ravel()])\n",
    "values = np.vstack([m1, m2])\n",
    "kernel = stats.gaussian_kde(values)\n",
    "Z = np.reshape(kernel(positions).T, X.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(np.rot90(Z), cmap=plt.cm.gist_earth_r,\n",
    "          extent=[xmin, xmax, ymin, ymax])\n",
    "ax.plot(m1, m2, 'k.', markersize=2)\n",
    "ax.set_xlim([xmin, xmax])\n",
    "ax.set_ylim([ymin, ymax])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fcdba2-3c1a-434a-a21e-345b1cdd5068",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "X.ravel()\n",
    "values\n",
    "kernel\n",
    "positions = np.vstack([X.ravel(), Y.ravel()])\n",
    "\n",
    "# plt.scatter(positions[0],positions[1],c='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69428698-c099-4a43-85bc-69d0f236699d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([11,22,55,33,77,88,11,22])\n",
    "y = np.array([69,90,1,34,33,88,48,22])\n",
    "phi = np.array([66,99,11,33,33,88,44,22])\n",
    "\n",
    "OK = OrdinaryKriging(x,y,phi, variogram_model='exponential',verbose = True, enable_plotting=True,)\n",
    "xx = np.arange(x.min(),x.max(),0.1)\n",
    "yy = np.arange(y.min(),y.max(),0.1)\n",
    "\n",
    "zstar, ss = OK.execute(\"grid\",xx,yy)\n",
    "\n",
    "plt.imshow(zstar,extent=(xx[0],xx[-1],yy[0],yy[-1]))\n",
    "plt.scatter(x,np.flip(y),c='k',marker='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564476b2-da47-412d-9e3d-efeb108e6b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ss,extent=(xx[0],xx[-1],yy[0],yy[-1]))\n",
    "\n",
    "y\n",
    "plt.scatter(np.flip(x),np.flip(y),c='k',marker='.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1980a1c8-1f6e-46ad-a6bd-41e7cbb9ff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dat.iloc[vv]\n",
    "points_in_area = dat[dat['Id'].isin(vv)]\n",
    "\n",
    "\n",
    "indexes_in_area = [ names_mapping[i] for i in list(points_in_area['Vitenskapelig navn'].unique())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba3e526-58a3-43c2-b3fc-df93ea1a36ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548f22dd-2e35-4600-9881-8b10582944ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_in_area\n",
    "\n",
    "\n",
    "stats_distance = norm(\n",
    "    loc=0, \n",
    "    scale=100\n",
    ")\n",
    "\n",
    "points_in_area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b61859-64a7-45f2-9b7b-f2da74945130",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones((185,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d021bc6-32e4-4f79-83b7-f39be6399e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.topk(5, 1, True, True))\n",
    "\n",
    "print(out.topk(5, 1, True, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b02d7a-15a5-44e1-94c0-6e41caeee656",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.ones((2,185))\n",
    "\n",
    "a = torch.tensor(b)\n",
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0067143-9ef8-4bf2-ab85-9edc438f9a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_species_distributions\n",
    "\n",
    "data = fetch_species_distributions()\n",
    "\n",
    "# Get matrices/arrays of species IDs and locations\n",
    "latlon = np.vstack([data.train['dd lat'],\n",
    "                    data.train['dd long']]).T\n",
    "species = np.array([d.decode('ascii').startswith('micro')\n",
    "                    for d in data.train['species']], dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585c4e16-95de-4248-87ec-a6b0919d0770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade matplotlib\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from sklearn.datasets import fetch_species_distributions\n",
    "\n",
    "def construct_grids(batch):\n",
    "    \"\"\"Construct the map grid from the batch object\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch : Batch object\n",
    "        The object returned by :func:`fetch_species_distributions`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (xgrid, ygrid) : 1-D arrays\n",
    "        The grid corresponding to the values in batch.coverages\n",
    "    \"\"\"\n",
    "    # x,y coordinates for corner cells\n",
    "    xmin = batch.x_left_lower_corner + batch.grid_size\n",
    "    xmax = xmin + (batch.Nx * batch.grid_size)\n",
    "    ymin = batch.y_left_lower_corner + batch.grid_size\n",
    "    ymax = ymin + (batch.Ny * batch.grid_size)\n",
    "\n",
    "    # x coordinates of the grid cells\n",
    "    xgrid = np.arange(xmin, xmax, batch.grid_size)\n",
    "    # y coordinates of the grid cells\n",
    "    ygrid = np.arange(ymin, ymax, batch.grid_size)\n",
    "\n",
    "    return (xgrid, ygrid)\n",
    "\n",
    "\n",
    "xgrid, ygrid = construct_grids(data)\n",
    "\n",
    "# plot coastlines with basemap\n",
    "m = Basemap(projection='cyl', resolution='c',\n",
    "            llcrnrlat=ygrid.min(), urcrnrlat=ygrid.max(),\n",
    "            llcrnrlon=xgrid.min(), urcrnrlon=xgrid.max())\n",
    "m.drawmapboundary(fill_color='#DDEEFF')\n",
    "m.fillcontinents(color='#FFEEDD')\n",
    "m.drawcoastlines(color='gray', zorder=2)\n",
    "m.drawcountries(color='gray', zorder=2)\n",
    "\n",
    "# plot locations\n",
    "m.scatter(latlon[:, 1], latlon[:, 0], zorder=3,\n",
    "          c=species, cmap='rainbow', latlon=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72577acf-46c4-4b0a-9ce7-c9cf65248b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3ea4ec-0503-462c-a9cc-a45c2e2c7d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sklearn\n",
    "from sklearn.neighbors import KernelDensity\n",
    "# Set up the data grid for the contour plot\n",
    "X, Y = np.meshgrid(xgrid[::5], ygrid[::5][::-1])\n",
    "land_reference = data.coverages[6][::5, ::5]\n",
    "land_mask = (land_reference > -9999).ravel()\n",
    "xy = np.vstack([Y.ravel(), X.ravel()]).T\n",
    "xy = np.radians(xy[land_mask])\n",
    "\n",
    "# Create two side-by-side plots\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "fig.subplots_adjust(left=0.05, right=0.95, wspace=0.05)\n",
    "species_names = ['Bradypus Variegatus', 'Microryzomys Minutus']\n",
    "cmaps = ['Purples', 'Reds']\n",
    "\n",
    "for i, axi in enumerate(ax):\n",
    "    axi.set_title(species_names[i])\n",
    "    \n",
    "    # plot coastlines with basemap\n",
    "    m = Basemap(projection='cyl', llcrnrlat=Y.min(),\n",
    "                urcrnrlat=0, llcrnrlon=200,\n",
    "                urcrnrlon=100, resolution='c', ax=axi)\n",
    "                # urcrnrlat=Y.max(), llcrnrlon=X.min(),\n",
    "                # urcrnrlon=X.max(), resolution='c', ax=axi)\n",
    "    m.drawmapboundary(fill_color='#DDEEFF')\n",
    "    m.drawcoastlines()\n",
    "    m.drawcountries()\n",
    "    \n",
    "    # construct a spherical kernel density estimate of the distribution\n",
    "    kde = KernelDensity(bandwidth=0.03, metric='haversine')\n",
    "    kde.fit(np.radians(latlon[species == i]))\n",
    "\n",
    "    # evaluate only on the land: -9999 indicates ocean\n",
    "    Z = np.full(land_mask.shape[0], -9999.0)\n",
    "    Z[land_mask] = np.exp(kde.score_samples(xy))\n",
    "    Z = Z.reshape(X.shape)\n",
    "\n",
    "    # plot contours of the density\n",
    "    levels = np.linspace(0, Z.max(), 25)\n",
    "    axi.contourf(X, Y, Z, levels=levels, cmap=cmaps[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dbf3c9-bdac-4a6e-97b8-a9e00ee6ddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "latlon[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b79b67-d44a-4dc8-bf4c-3358d332a4cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896f2f05-c375-4353-afb6-49c03fa506fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "map = Basemap(projection='ortho',lat_0=45,lon_0=-100,resolution='l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b49753-6299-4f9c-9016-ce08df7f85c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# set up orthographic map projection with\n",
    "# perspective of satellite looking down at 50N, 100W.\n",
    "# use low resolution coastlines.\n",
    "map = Basemap(projection='cyl',lat_0=0,lon_0=0,resolution='l')\n",
    "# draw coastlines, country boundaries, fill continents.\n",
    "map.drawcoastlines(linewidth=0.25)\n",
    "map.drawcountries(linewidth=0.25)\n",
    "map.fillcontinents(color='coral',lake_color='aqua')\n",
    "# draw the edge of the map projection region (the projection limb)\n",
    "map.drawmapboundary(fill_color='aqua')\n",
    "# draw lat/lon grid lines every 30 degrees.\n",
    "# map.drawmeridians(np.arange(0,360,30))\n",
    "# map.drawparallels(np.arange(-90,90,30))\n",
    "# make up some data on a regular lat/lon grid.\n",
    "nlats = 73; nlons = 145; delta = 2.*np.pi/(nlons-1)\n",
    "lats = (0.5*np.pi-delta*np.indices((nlats,nlons))[0,:,:])\n",
    "lons = (delta*np.indices((nlats,nlons))[1,:,:])\n",
    "wave = 0.75*(np.sin(2.*lats)**8*np.cos(4.*lons))\n",
    "mean = 0.5*np.cos(2.*lats)*((np.sin(2.*lats))**2 + 2.)\n",
    "# compute native map projection coordinates of lat/lon grid.\n",
    "x, y = map(lons*180./np.pi, lats*180./np.pi)\n",
    "# contour data over the map.\n",
    "cs = map.contour(x,y,wave+mean,15,linewidths=1.5)\n",
    "plt.title('contour lines over filled continent background')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d99368f-fd93-4298-b0f2-cb3ba9149386",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat['Vitenskapelig navn'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4888f578-f2b7-4bbb-912e-8a36df5c0f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dat[dat['Vitenskapelig navn']=='Stellaria nemorum']#'Lupinus polyphyllus']\n",
    "\n",
    "\n",
    "ll = d[['lat','long']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b05d47-37bd-4d4d-bbcd-eae5d1c79983",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0b1ede-51ea-4d45-b2fe-e346eb951a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgrid = np.arange(3,34,0.1)\n",
    "ygrid = np.arange(57,72,(72-57)/len(xgrid))\n",
    "\n",
    "\n",
    "X, Y = np.meshgrid(xgrid[::5], ygrid[::5][::-1])\n",
    "xy = np.vstack([Y.ravel(), X.ravel()]).T\n",
    "# xy = np.radians(xy[land_mask])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a394a3-bab3-4f1e-8a74-918b4554fac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "map = Basemap(llcrnrlon=3.,llcrnrlat=57.,urcrnrlon=34.,urcrnrlat=72.,\n",
    "             resolution='i', lat_0 = 39.5, lon_0 = -3.25)\n",
    "# map.drawmapboundary(fill_color='blue')\n",
    "# map.drawcountries(linewidth=0.25)\n",
    "map.fillcontinents(color='white')#,lake_color='blue')\n",
    "# map.scatter(d['long'],d['lat'], zorder=3, cmap='rainbow', latlon=True);\n",
    "\n",
    "map.drawmapboundary(fill_color='#DDEEFF')\n",
    "map.drawcoastlines()\n",
    "map.drawcountries()\n",
    "\n",
    "kde = KernelDensity(bandwidth=0.03, metric='haversine')\n",
    "kde.fit(ll)\n",
    "\n",
    "# evaluate only on the land: -9999 indicates ocean\n",
    "# Z = np.full(land_mask.shape[0], -9999.0)\n",
    "Z = np.exp(kde.score_samples(xy))\n",
    "Z = Z.reshape(X.shape)\n",
    "\n",
    "Z=Z*100\n",
    "# plot contours of the density\n",
    "levels = np.linspace(0, Z.max(), 25)\n",
    "\n",
    "axi.contourf(X, Y, Z, levels=levels, cmap='Reds')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f2772b-3b52-411f-a54d-53306d36a199",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.subplots_adjust(left=0.05, right=0.95, wspace=0.05)\n",
    "species_names = ['Bradypus Variegatus', 'Microryzomys Minutus']\n",
    "cmaps = ['Purples', 'Reds']\n",
    "ax=[ax]\n",
    "for i, axi in enumerate(ax):\n",
    "    axi.set_title(species_names[i])\n",
    "    \n",
    "    # plot coastlines with basemap\n",
    "    # m = Basemap(projection='cyl', llcrnrlat=Y.min(),\n",
    "    #             urcrnrlat=Y.max(), llcrnrlon=X.min(),\n",
    "    #             urcrnrlon=X.max(), resolution='c', ax=axi)\n",
    "    m = Basemap(llcrnrlon=3.,llcrnrlat=57.,urcrnrlon=34.,urcrnrlat=72.,\n",
    "             resolution='i', lat_0 = 39.5, lon_0 = -3.25)\n",
    "    m.drawmapboundary(fill_color='#DDEEFF')\n",
    "    m.drawcoastlines()\n",
    "    m.drawcountries()\n",
    "    m.scatter(d['long'],d['lat'], zorder=3, cmap='rainbow', latlon=True);\n",
    "    # construct a spherical kernel density estimate of the distribution\n",
    "    kde = KernelDensity(bandwidth=0.5)\n",
    "    kde.fit(ll)\n",
    "\n",
    "    # evaluate only on the land: -9999 indicates ocean\n",
    "    # Z = np.full(land_mask.shape[0], -9999.0)\n",
    "    Z = np.exp(kde.score_samples(xy))\n",
    "    Z = Z.reshape(X.shape)\n",
    " \n",
    "    # plot contours of the density\n",
    "    levels = np.linspace(0, Z.max(), 25)\n",
    "    axi.contourf(X, Y, Z, levels=levels, cmap=cmaps[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8c1767-a417-448f-a125-776a6701bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the data grid for the contour plot\n",
    "xgrid, ygrid = construct_grids(data)\n",
    "\n",
    "xgrid = np.arange(3,34,0.1)\n",
    "ygrid = np.arange(57,72,(72-57)/len(xgrid))\n",
    "\n",
    "\n",
    "X, Y = np.meshgrid(xgrid[::5], ygrid[::5][::-1])\n",
    "land_reference = data.coverages[6][::5, ::5]\n",
    "\n",
    "xy = np.vstack([Y.ravel(), X.ravel()]).T\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax = [ax] \n",
    "fig.subplots_adjust(left=0.05, right=0.95, wspace=0.05)\n",
    "species_names = ['Bradypus Variegatus', 'Microryzomys Minutus']\n",
    "cmaps = ['Purples', 'Reds']\n",
    "for i, axi in enumerate(ax):\n",
    "    axi.set_title(species_names[i])\n",
    "    \n",
    "    # plot coastlines with basemap\n",
    "    # m = Basemap(projection='cyl', llcrnrlat=Y.min(),\n",
    "    #             urcrnrlat=Y.max(), llcrnrlon=X.min(),\n",
    "    #             urcrnrlon=X.max(), resolution='c', ax=axi)\n",
    "\n",
    "    map = Basemap(llcrnrlon=3.,llcrnrlat=57.,urcrnrlon=34.,urcrnrlat=72.,\n",
    "             resolution='i', lat_0 = 39.5, lon_0 = -3.25)\n",
    "    map.drawmapboundary(fill_color='#DDEEFF')\n",
    "    map.drawcoastlines()\n",
    "    map.drawcountries()\n",
    "    \n",
    "    # construct a spherical kernel density estimate of the distribution\n",
    "    kde = KernelDensity(bandwidth=1)\n",
    "    kde.fit(ll)\n",
    "    \n",
    "    # evaluate only on the land: -9999 indicates ocean\n",
    "    \n",
    "\n",
    "    Z= np.exp(kde.score_samples(xy))\n",
    "\n",
    "    Z = Z.reshape(X.shape)\n",
    "\n",
    "    \n",
    "    # map.scatter(d['long'],d['lat'], zorder=3, latlon=True);\n",
    "    \n",
    "    # plot contours of the density\n",
    "    levels = np.linspace(0, Z.max(), 15)\n",
    "    axi.contourf(X, Y, Z, levels=levels, cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00ccd7c-9e64-4b6a-b20e-51ce4aa6fd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06213078-10b6-48a8-bbb1-f24d375f380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kde.score_samples(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13241002-2710-4990-84e2-0bb76a4649b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4b8729-51ab-4d89-b6a9-0554eecde90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "within_square \n",
    "sample_pos \n",
    "output_aug_kde001 \n",
    "weight_tensor_kde001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9daa12b-c6f3-4967-9236-bd43a5b6c042",
   "metadata": {},
   "outputs": [],
   "source": [
    "within_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5322c2da-f567-45b7-b790-3d6deadd4d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(within_square['Vitenskapelig navn'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b9d84f-a827-4327-a93c-e7187bd337a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "torch.exp(output_aug_kde001[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bf4a67-22f1-4bf1-942e-f738dc7f582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def kernel_density_estimate_value(point_row,dat,bandwidth = 1):\n",
    "    # print(point_row)\n",
    "    # print(dat)\n",
    "    if point_row.index[0] in list(dat.index):\n",
    "        np_dat_lat_long = dat.drop(point_row.index[0])[['lat','long']].to_numpy()\n",
    "    else:\n",
    "        np_dat_lat_long = dat[['lat','long']].to_numpy()\n",
    "    kde = KernelDensity(bandwidth=bandwidth)\n",
    "    # print(np_dat_lat_long)\n",
    "    if len(np_dat_lat_long)==0:\n",
    "        return 1\n",
    "    kde.fit(np_dat_lat_long)\n",
    "    np_point = np.array([[point_row['lat'].iloc[0],point_row['long'].iloc[0]]])\n",
    "    return np.exp(kde.score_samples(np_point))[0]\n",
    "\n",
    "# kernel_density_estimate_value()\n",
    "within_square_ = within_square[within_square['Vitenskapelig navn']== 'Hypericum maculatum']\n",
    "\n",
    "kernel_density_estimate_value(sample_pos,within_square_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4aa4ec-5edb-42ce-ac4a-2a5c4ce5849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_tensor_kde001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475ce88b-9fc5-424d-b259-50cf552e0534",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_dat_lat_long = within_square_[['lat','long']].to_numpy()\n",
    "kde = KernelDensity(bandwidth=1)\n",
    "# print(np_dat_lat_long)\n",
    "\n",
    "kde.fit(np_dat_lat_long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b48b806-5189-4e7f-ad81-897b380edd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "kde.score(np.array([[sample_pos['lat'].iloc[0],sample_pos['long'].iloc[0]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c9b9dc-1e57-418f-b325-fcc9636f9028",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_tensor_kde001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527635f9-67b8-441a-8f10-6c67aa07e579",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_tensor_kde001.min()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cfa36c-07fb-4e0a-8d2d-d37357d35121",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_tensor_kde001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25585c0f-641c-4472-ba72-ccbcaceb816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_tensor_kde001.sum()+159"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde9f256-46ac-478d-b33e-d94d48e1c277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install geopandas\n",
    "import urllib\n",
    "import geojson\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "import json\n",
    "\n",
    "# kilde https://data.artsdatabanken.no/Natur_i_Norge/Landskap/Typeinndeling\n",
    "# data_geo = gpd.read_file('dataset/landskapsgradient.geojson')\n",
    "\n",
    "\n",
    "# f = open('dataset/landskapsgradient.geojson')\n",
    "# a =json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bf2ee4-4223-405e-81be-7a0a2bf85db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# p = Point(89100,6973100)\n",
    "\n",
    "# # p2.within(poly)\n",
    "# p.within(data['geometry'][0])\n",
    "# data\n",
    "print(a.keys())\n",
    "\n",
    "# a['features'][300]\n",
    "p = Polygon(a['features'][0]['geometry']['coordinates'])\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee0110a-9b05-4b74-af53-4a75d9fd60c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from owslib.wms import WebMapService\n",
    "# S&version=1.1.1&\n",
    "wms = WebMapService('https://wms.artsdatabanken.no/?map=/maps/mapfiles/la.map&?&request=GetCapabilities&service=WM', version='1.1.1')\n",
    "\n",
    "from pyproj import Proj, transform\n",
    "import json\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d8b8ee-3c42-4934-b9f9-39bb28dd537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = wms.contents['LA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217ca220-052d-4b50-bd23-e008f1154ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wms.getfeatureinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542d2810-da74-4097-b984-712f35d12ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    " img = wms.getfeatureinfo(    \n",
    "      layers=['LA'], \n",
    "      styles=['default'], \n",
    "      srs='EPSG:32633', \n",
    "      bbox=(245953+1,6654310+1,245953-1,6654310-1), \n",
    "      size=(600, 300), \n",
    "      format='application/json', \n",
    "        xy=(0,0),\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a55a548-60c8-4c46-8990-ca63f2a2a881",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5075b345-cc8f-45a5-95a4-32071401d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv('dataset/lokasjon_top_185_with_height_flora.csv')\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd410de2-df40-4ad1-bf6e-bae79ee60136",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outProj = Proj(init='epsg:32633') \n",
    "\n",
    "inProj = Proj(init='epsg:32633') \n",
    "region_size=(100,100)\n",
    "\n",
    "p = (245953,6654310)\n",
    "\n",
    "\n",
    "loc = p\n",
    "\n",
    "x, y = transform(inProj,outProj,loc[0],loc[1])\n",
    "\n",
    "xupper = int(round(x - region_size[0] / 2))\n",
    "xlower = int(round(x + region_size[0] / 2))\n",
    "yupper = int(round(y - region_size[1] / 2))\n",
    "ylower = int(round(y + region_size[1] / 2))\n",
    "bbox = (xupper, yupper, xlower, ylower)\n",
    "#bbox=(-11375363,3757392,-11372226,3759102)\n",
    "size=region_size\n",
    "size=(1333, 716)\n",
    "\n",
    "wms_version = '1.1.1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f7b873-5faf-4001-bcae-3f55197ca560",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f035b6-73d7-4e04-b604-a972f87e777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = wms.getmap(\n",
    "    layers=['1_NATURAL_COL0R'],\n",
    "    srs='EPSG:32633',\n",
    "    bbox = bbox,\n",
    "    size = size,\n",
    "    info_format='application/json',\n",
    "    xy = (0, 0)\n",
    ")\n",
    "# info = json.loads(info.read())\n",
    "info.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edcb9dd-3eed-4f63-a5c5-169989941f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# flora_list = []\n",
    "# kode_list = []\n",
    "# index_list = []\n",
    "ii = 446376\n",
    "for i,p in enumerate(tqdm(dat['points'][ii:])):\n",
    "    point = Point(p[0],p[1])\n",
    "    for g, flora, kode in zip(data_geo['geometry'],data_geo['navn'],data_geo['kode']):\n",
    "        if point.within(g):\n",
    "            flora_list.append(flora)\n",
    "            kode_list.append(kode)\n",
    "            index_list.append(i+ii)\n",
    "            break \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e4ded0-df95-43bc-81e2-232dca9fcc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(kode_list))\n",
    "# print(i)\n",
    "# index_list[-1]+\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "b =pd.read_csv('nature_type_geo.csv')\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b369c64-f872-4ec5-a1be-8d5464020ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame([])\n",
    "df['kode']= kode_list\n",
    "df['index'] = index_list\n",
    "df['flora'] = flora_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ac7b4c-f0ae-48c5-a1fa-8ec36d2826ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('nature_type_geo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37af91d4-0402-4b36-9f91-aa830cf7467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kode_list = kode_list[:417604-874]\n",
    "# index_list = index_list[:417604-874]\n",
    "# flora_list = flora_list[:417604-874]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de3dc08-0598-4efa-9905-a9d4dbd837b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f3ac4e-6ed8-48f3-a039-3998b0645ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fc87bb-80bd-49f1-bb33-232c6f7ddac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    s = [1,2]\n",
    "    return s.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ae90f8-49da-4d88-bc82-3584055d8a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c= f(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae44da0f-8965-4b40-a2f0-91f4acfcc3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dat = pd.read_csv('dataset/lokasjon_top_185_with_height.csv')\n",
    "dat2 = pd.read_csv('dataset/nature_type_geo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb385de6-edb4-4d1b-b311-bc681e6c2ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dat['Unnamed: 0'])\n",
    "len(dat2['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3459967-e4ec-4adb-918f-95479d7c8f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat2=dat2[['flora','kode','index']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4169ef3-d866-47fb-b06c-76f2d69ebe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat3 =pd.merge(\n",
    "    dat,\n",
    "    dat2,\n",
    "    how=\"left\",\n",
    "    left_on ='Unnamed: 0',\n",
    "    right_on = 'index'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ad733f-e692-4335-90a1-84526bfbf2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dat3['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad02cc1f-bded-4055-b4e1-8517ac76bf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dat['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fc9590-e8ff-424b-a49f-0b0d85640486",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2ffb56-9893-4390-8f64-fb809488464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = dat[['Vitenskapelig navn','height']].groupby('Vitenskapelig navn')\n",
    "\n",
    "grouped.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0b2861-a164-4e23-9839-e0600fd7e62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "v =grouped.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7319e927-abf6-42f7-bc84-24fd20e14e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "v.loc['Acer platanoides'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ca53c1-184f-41cf-a5ed-d5693c01e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afecb7d-782b-4a60-a992-0dfaa9def127",
   "metadata": {},
   "outputs": [],
   "source": [
    "flora_ = list(dat2['flora'])\n",
    "kode_ = list(dat2['kode'])\n",
    "index_ = list(dat2['index'])\n",
    "\n",
    "kode = ['U'] * len(list(dat['Id']))\n",
    "flora = ['unknown'] * len(list(dat['Id']))\n",
    "\n",
    "# index = list(dat[''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bd70e3-5949-4f10-b588-3ad0304b5b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in enumerate(index_):\n",
    "    flora[j] = flora_[i]\n",
    "    kode[j] = kode_[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08a05e6-655d-4bed-a693-75d7c37c0783",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat['flora'] = flora\n",
    "dat['kode'] = kode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdcbf0e-61ed-43ae-90f2-d66ae8cc63a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f043a7d-5d30-4cc4-9816-78b09f21e6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.from_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acda5bdd-88a7-4a96-a892-3494cd04eef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
